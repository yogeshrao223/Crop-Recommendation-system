{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a37ff363-0765-439f-a185-0215b0444457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "\n",
      "First 5 rows of the dataset:\n",
      "   Temperature   Humidity    Rainfall        PH  Nitrogen  Phosphorous  \\\n",
      "0    20.879744  82.002744  202.935536  6.502985     69.30     79.50000   \n",
      "1    21.770462  80.319644  226.655537  7.038096     72.02    141.82400   \n",
      "2    23.004459  82.320763  263.964248  7.633568     77.77     59.39000   \n",
      "3    26.491096  80.283629  242.864034  6.980401     78.65    147.45895   \n",
      "4    20.280071  81.604873  262.717340  7.628473     73.98     68.95000   \n",
      "\n",
      "   Potassium  Crop  \n",
      "0    94.4400  rice  \n",
      "1   141.6978  rice  \n",
      "2    81.8900  rice  \n",
      "3   142.9430  rice  \n",
      "4    95.7400  rice  \n",
      "\n",
      "Original classes: ['Adzuki Beans' 'Black gram' 'Chickpea' 'Coconut' 'Coffee' 'Cotton'\n",
      " 'Ground Nut' 'Jute' 'Kidney Beans' 'Lentil' 'Moth Beans' 'Mung Bean'\n",
      " 'Peas' 'Pigeon Peas' 'Rubber' 'Sugarcane' 'Tea' 'Tobacco' 'apple'\n",
      " 'banana' 'grapes' 'maize' 'mango' 'millet' 'muskmelon' 'orange' 'papaya'\n",
      " 'pomegranate' 'rice' 'watermelon' 'wheat']\n",
      "Total number of features: 7\n",
      "Total number of samples: 3100\n",
      "\n",
      "Training set size: 2480 samples (Scaled and converted to float32)\n",
      "Test set size: 620 samples (Scaled and converted to float32)\n",
      "\n",
      "Starting TabNet model training with early stopping...\n",
      "\n",
      "Early stopping occurred at epoch 78 with best_epoch = 68 and best_val_0_accuracy = 0.81855\n",
      "Model training complete.\n",
      "\n",
      "--- Model Evaluation Results ---\n",
      "Accuracy on Test Set: 80.81%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Adzuki Beans       1.00      1.00      1.00        20\n",
      "  Black gram       0.89      0.80      0.84        20\n",
      "    Chickpea       1.00      1.00      1.00        20\n",
      "     Coconut       1.00      0.90      0.95        20\n",
      "      Coffee       0.84      0.80      0.82        20\n",
      "      Cotton       0.62      0.80      0.70        20\n",
      "  Ground Nut       0.90      0.90      0.90        20\n",
      "        Jute       0.68      0.95      0.79        20\n",
      "Kidney Beans       1.00      1.00      1.00        20\n",
      "      Lentil       0.95      1.00      0.98        20\n",
      "  Moth Beans       0.95      0.95      0.95        20\n",
      "   Mung Bean       0.85      0.85      0.85        20\n",
      "        Peas       1.00      1.00      1.00        20\n",
      " Pigeon Peas       0.62      0.40      0.48        20\n",
      "      Rubber       0.90      0.95      0.93        20\n",
      "   Sugarcane       0.85      0.55      0.67        20\n",
      "         Tea       0.82      0.90      0.86        20\n",
      "     Tobacco       0.76      0.95      0.84        20\n",
      "       apple       0.79      0.95      0.86        20\n",
      "      banana       0.71      0.85      0.77        20\n",
      "      grapes       0.88      0.70      0.78        20\n",
      "       maize       0.56      0.45      0.50        20\n",
      "       mango       0.62      0.80      0.70        20\n",
      "      millet       1.00      1.00      1.00        20\n",
      "   muskmelon       1.00      0.90      0.95        20\n",
      "      orange       0.61      0.55      0.58        20\n",
      "      papaya       0.71      0.60      0.65        20\n",
      " pomegranate       0.60      0.60      0.60        20\n",
      "        rice       0.82      0.70      0.76        20\n",
      "  watermelon       0.87      1.00      0.93        20\n",
      "       wheat       0.29      0.25      0.27        20\n",
      "\n",
      "    accuracy                           0.81       620\n",
      "   macro avg       0.81      0.81      0.80       620\n",
      "weighted avg       0.81      0.81      0.80       620\n",
      "\n",
      "\n",
      "--- Single Prediction Example ---\n",
      "Input Features (Unscaled):\n",
      "{'Temperature': 13.28504331, 'Humidity': 83.54193816, 'Rainfall': 65.80006004, 'PH': 5.69945282, 'Nitrogen': 54.06, 'Phosphorous': 49.27, 'Potassium': 46.75}\n",
      "Predicted Crop: grapes\n",
      "True Crop (for comparison): grapes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yogesh\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytorch_tabnet\\callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import torch # Imported as TabNet is built on PyTorch\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "# Assuming the file \"Dataset 1.csv\" is in the same directory as this script.\n",
    "try:\n",
    "    df = pd.read_csv(\"Dataset 1.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Dataset 1.csv' not found. Please ensure the file is in the correct path.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# The last column, 'Crop', is our target variable.\n",
    "X = df.drop('Crop', axis=1)\n",
    "y = df['Crop']\n",
    "\n",
    "# Check for missing values in features and handle them\n",
    "if X.isnull().sum().any():\n",
    "    print(\"\\nWarning: Missing values detected. Filling with median for simplicity.\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# --- 3. Preprocessing the Target Variable (Label Encoding) ---\n",
    "# TabNet, like XGBoost, uses integer encoding for multi-class targets.\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "print(f\"\\nOriginal classes: {class_names}\")\n",
    "print(f\"Total number of features: {X.shape[1]}\")\n",
    "print(f\"Total number of samples: {X.shape[0]}\")\n",
    "\n",
    "# --- 4. Split Data into Training and Testing Sets ---\n",
    "# We use 80% for training and 20% for testing, with a fixed random_state for reproducibility.\n",
    "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# --- 5. Feature Scaling (Crucial for Neural Networks like TabNet) ---\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
    "X_test_scaled = scaler.transform(X_test_raw)\n",
    "\n",
    "# Convert data to NumPy arrays with float32 type for TabNet\n",
    "X_train = X_train_scaled.astype(np.float32)\n",
    "X_test = X_test_scaled.astype(np.float32)\n",
    "y_train = y_train.astype(np.int64)\n",
    "y_test = y_test.astype(np.int64)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples (Scaled and converted to float32)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples (Scaled and converted to float32)\")\n",
    "\n",
    "\n",
    "# --- 6. Initialize and Train the TabNet Model ---\n",
    "# TabNetClassifier is used for classification tasks.\n",
    "# We use a small validation set from the training data for early stopping.\n",
    "model = TabNetClassifier(\n",
    "    n_steps=3,             # Number of steps in the architecture (boosted trees analogy)\n",
    "    gamma=1.5,             # Multiplicative factor for attention mechanism\n",
    "    n_d=8,                 # Dimension of the prediction layer (width)\n",
    "    n_a=8,                 # Dimension of the attention layer (width)\n",
    "    seed=42,\n",
    "    verbose=0              # Set to 1 to see epoch-by-epoch training logs\n",
    ")\n",
    "\n",
    "# Use a small validation set (10%) from the training data for TabNet's internal early stopping\n",
    "X_train_fit, X_val, y_train_fit, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(\"\\nStarting TabNet model training with early stopping...\")\n",
    "model.fit(\n",
    "    X_train_fit, y_train_fit,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=100,             # Set a reasonable number of epochs\n",
    "    patience=10,                # Stop training if validation metric does not improve after 10 epochs\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 7. Model Prediction and Evaluation ---\n",
    "\n",
    "# Predict on the test set. TabNet's predict returns the class index.\n",
    "y_pred_encoded = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(f\"\\n--- Model Evaluation Results ---\")\n",
    "print(f\"Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred_encoded,\n",
    "    target_names=class_names, # Use the original class names for readability\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# --- Example: Making a Single Prediction ---\n",
    "# You can now use the model to predict the crop for a new set of data\n",
    "# Note: Prediction must be made on the SCALED data\n",
    "sample_data = X_test[0, :]\n",
    "sample_data_input = np.array([sample_data]).astype(np.float32)\n",
    "\n",
    "single_prediction_encoded = model.predict(sample_data_input)[0]\n",
    "single_prediction_label = label_encoder.inverse_transform([single_prediction_encoded])[0]\n",
    "true_label = label_encoder.inverse_transform([y_test[0]])[0]\n",
    "\n",
    "print(\"\\n--- Single Prediction Example ---\")\n",
    "# To show the original (unscaled) input values, we use X_test_raw\n",
    "print(f\"Input Features (Unscaled):\\n{X_test_raw.iloc[0].to_dict()}\")\n",
    "print(f\"Predicted Crop: {single_prediction_label}\")\n",
    "print(f\"True Crop (for comparison): {true_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2752453-498b-4f45-939c-d56a485cd5ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'catboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcatboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CatBoostClassifier \u001b[38;5;66;03m# Swapped XGBoost for CatBoost\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'catboost'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier # Swapped XGBoost for CatBoost\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load the Dataset ---\n",
    "# Assuming the file \"Dataset 1.csv\" is in the same directory as this script.\n",
    "try:\n",
    "    df = pd.read_csv(\"Dataset 1.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the dataset:\")\n",
    "    print(df.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Dataset 1.csv' not found. Please ensure the file is in the correct path.\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Separate Features (X) and Target (y) ---\n",
    "# The last column, 'Crop', is our target variable.\n",
    "X = df.drop('Crop', axis=1)\n",
    "y = df['Crop']\n",
    "\n",
    "# Check for missing values in features and handle them\n",
    "if X.isnull().sum().any():\n",
    "    print(\"\\nWarning: Missing values detected. Filling with median for simplicity.\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# --- 3. Preprocessing the Target Variable (Label Encoding) ---\n",
    "# CatBoost, like other multi-class classifiers, works best with integer encoded targets.\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "print(f\"\\nOriginal classes: {class_names}\")\n",
    "print(f\"Total number of features: {X.shape[1]}\")\n",
    "print(f\"Total number of samples: {X.shape[0]}\")\n",
    "\n",
    "# --- 4. Split Data into Training and Testing Sets ---\n",
    "# We use 80% for training and 20% for testing, with a fixed random_state for reproducibility.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n",
    "\n",
    "# --- 5. Initialize and Train the CatBoost Model ---\n",
    "# CatBoostClassifier is used for classification tasks.\n",
    "model = CatBoostClassifier(\n",
    "    iterations=100,             # Equivalent to n_estimators in XGBoost\n",
    "    learning_rate=0.1,\n",
    "    loss_function='MultiClass', # Required for multi-class problems\n",
    "    random_seed=42,\n",
    "    verbose=0,                  # Suppress training output for cleaner execution\n",
    "    allow_writing_files=False   # Prevent CatBoost from writing auxiliary files\n",
    ")\n",
    "\n",
    "print(\"\\nStarting CatBoost model training...\")\n",
    "# Note: CatBoost does not require explicit feature scaling.\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# --- 6. Model Prediction and Evaluation ---\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_encoded = model.predict(X_test)\n",
    "# CatBoost's predict() returns a 2D array of predictions; we flatten it to 1D\n",
    "y_pred_encoded = y_pred_encoded.flatten()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_encoded)\n",
    "print(f\"\\n--- Model Evaluation Results ---\")\n",
    "print(f\"Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Generate a detailed classification report\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    y_pred_encoded,\n",
    "    target_names=class_names, # Use the original class names for readability\n",
    "    zero_division=0\n",
    ")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(report)\n",
    "\n",
    "# --- Example: Making a Single Prediction ---\n",
    "# You can now use the model to predict the crop for a new set of data\n",
    "sample_data = X_test.iloc[0] # Taking the first sample from the test set\n",
    "sample_data_df = pd.DataFrame([sample_data], columns=X.columns)\n",
    "\n",
    "single_prediction_encoded = model.predict(sample_data_df)[0][0] # CatBoost predict outputs [[value]]\n",
    "single_prediction_label = label_encoder.inverse_transform([single_prediction_encoded])[0]\n",
    "true_label = label_encoder.inverse_transform([y_test[0]])[0]\n",
    "\n",
    "print(\"\\n--- Single Prediction Example ---\")\n",
    "print(f\"Input Features:\\n{sample_data.to_dict()}\")\n",
    "print(f\"Predicted Crop: {single_prediction_label}\")\n",
    "print(f\"True Crop (for comparison): {true_label}\")\n",
    "\n",
    "# --- Feature Importance Visualization (Text-based) ---\n",
    "# This shows which features were most influential in the model's decision-making.\n",
    "feature_importances = model.get_feature_importance()\n",
    "feature_names = X.columns\n",
    "sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "print(\"\\n--- Feature Importances (Top 5) ---\")\n",
    "for i in range(min(5, len(feature_names))):\n",
    "    feature = feature_names[sorted_idx[i]]\n",
    "    importance = feature_importances[sorted_idx[i]]\n",
    "    print(f\"  {i+1}. {feature}: {importance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6db15b-c78e-431c-a23a-94f39adf86e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install catboost"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
